iris3<-cbind(iris2, predictLDA$x[,1:2]) # añadimos el iris2 a las que acabamos de obtener
partimat(Species ~ LD2+LD1, data=iris3, method="lda")
partimat(Species ~., data=iris2, method="lda", plot.matrix=TRUE)
## Uso de la libreria ggplot y representacion simultanea de datos y contornos de densidad (estimaci�n kernel)
library(ggplot2)
quickplot(iris3$LD1,iris3$LD2, colour=iris3$Species, geom=c("point","density2d"))
## Analisis Discriminante Cuadr�tico (qda).
## Estimaci�n de la matriz de mala clasificaci�n y del error de mala clasificacion aparente (peligro de optimismo)
mod.qda <- qda(Species ~ ., data = iris2)
mod.qda
TAB <- table(iris2$Species, predictQDA$class)
predictQDA <- predict(mod.qda)
TAB <- table(iris2$Species, predictQDA$class)
TAB
mcrqda <- 1 - sum(diag(TAB))/sum(TAB)
mcrqda
## Representamos  de forma simultanea la frontera y los datos (utilizamos las variables can�nicas de discriminacion)
## Uso de la funci�n partimat de la libreria klaR
library(klaR)
partimat(Species ~ ., data=iris2, method="qda", plot.matrix=TRUE)
## Naive Bayes (e1071). En v. métricas asume normalidad. Admite suavizado de laplace.
mod.NB <- naiveBayes(Species ~ ., data = iris2)
mod.NB
library(e1071)
mod.NB <- naiveBayes(Species ~ ., data = iris2)
mod.NB
## Estimaci�n de la matriz de mala clasificaci�n y del error de mala clasificacion aparente (peligro de optimismo)
predictNB <- predict(mod.NB, newdata=iris2)
TAB <- table(iris2$Species, predictNB)
TAB
mcrNB <- 1 - sum(diag(TAB))/sum(TAB)
mcrNB
library(klaR)
partimat(Species ~ ., data=iris2, method="qda", plot.matrix=TRUE)
## Naive Bayes (e1071). En v. métricas asume normalidad. Admite suavizado de laplace.
library(e1071)
mod.NB <- naiveBayes(Species ~ ., data = iris2)
mod.NB
predictNB <- predict(mod.NB, newdata=iris2)
TAB <- table(iris2$Species, predictNB)
TAB
mcrNB <- 1 - sum(diag(TAB))/sum(TAB)
mcrNB
partimat(Species ~., data=iris2, method="naiveBayes", plot.matrix=TRUE)
## EStimacion por knn  (k=3 default)
mod.sknn <- sknn(Species ~ ., data = iris2, gamma=0)
mod.sknn
library(e1071)
mod.NB <- naiveBayes(Species ~ ., data = iris2)
mod.NB
# Naive Bayes (e1071). En v. métricas asume normalidad. Admite suavizado de laplace.
library(e1071)
mod.NB <- naiveBayes(Species ~ ., data = iris2)
mod.NB
predictNB <- predict(mod.NB, newdata=iris2)
TAB <- table(iris2$Species, predictNB)
TAB
mcrNB <- 1 - sum(diag(TAB))/sum(TAB)
mcrNB
partimat(Species ~., data=iris2, method="naiveBayes", plot.matrix=TRUE)
# construye un clasificador basandose en los3 vecinos próximos
mod.sknn <- sknn(Species ~ ., data = iris2, gamma=0)
mod.sknn
predictsknn <- predict(mod.sknn, newdata=iris2)
TAB <- table(iris2$Species, predictsknn$class)
TAB
mcrsknn <- 1 - sum(diag(TAB))/sum(TAB)
mcrsknn
partimat(Species ~., data=iris2, method="sknn", plot.matrix=TRUE, gamma=1.0)
predictsknn <- predict(mod.sknn, newdata=iris2)
TAB <- table(iris2$Species, predictsknn$class)
TAB
mcrsknn <- 1 - sum(diag(TAB))/sum(TAB)
mcrsknn
# gamma es el suavizado, gamma = 0.0 no suaviza
partimat(Species ~., data=iris2, method="sknn", plot.matrix=TRUE, gamma=0.0)
predictsknn <- predict(mod.sknn, newdata=iris2)
TAB <- table(iris2$Species, predictsknn$class)
TAB
mcrsknn <- 1 - sum(diag(TAB))/sum(TAB)
mcrsknn
# gamma es el suavizado, gamma = 0.0 no suaviza
partimat(Species ~., data=iris2, method="sknn", plot.matrix=TRUE, gamma=0.0)
# permite hacer una selección secuencial de las variables más relevantes
# Lineal
stepclass(Species~., data=iris2, method="lda", improvement=0.001)
# Cuadr�tico
stepclass(Species~., data=iris2, method="qda", improvement=0.001)
x <- iris2[,-5]
y <- iris2[,5]
obj2 <- tune.knn(x,y, k = 1:15, tunecontrol = tune.control(sampling = "boot",nboot=500))
summary(obj2)
plot(obj2)
tune.knn(x,y, k = 12, tunecontrol = tune.control(sampling = "boot",nboot=500))
obj2 <- tune.knn(x,y, k = 1:15, tunecontrol = tune.control(sampling = "boot",nboot=500))
summary(obj2)
plot(obj2)
tune.knn(x,y, k = 12, tunecontrol = tune.control(sampling = "boot",nboot=500))
obj2 <- tune.knn(x,y, k = 1:15, tunecontrol = tune.control(sampling = "boot",nboot=900))
summary(obj2)
plot(obj2)
tune.knn(x,y, k = 12, tunecontrol = tune.control(sampling = "boot",nboot=500))
tune.knn(x,y, k = 12, tunecontrol = tune.control(sampling = "boot",nboot=500))
plot(obj2)
tune.knn(x,y, k = 12, tunecontrol = tune.control(sampling = "boot",nboot=500))
summary(obj2)
plot(obj2)
confusion <- function(actual, predicted, names=NULL,
printit=TRUE, prior=NULL){
if(is.null(names))names <- levels(actual)
tab <- table(actual, predicted)
acctab <- t(apply(tab, 1, function(x)x/sum(x)))
dimnames(acctab) <- list(Actual=names,
"Predicted (cv)"=names)
if(is.null(prior)){
relnum <- table(actual)
prior <- relnum/sum(relnum)
acc <- sum(tab[row(tab)==col(tab)])/sum(tab)
} else
{
acc <- sum(prior*diag(acctab))
names(prior) <- names
}
if(printit)print(round(c("Overall accuracy"=acc,
"Prior frequency"=prior),4))
if(printit){    cat("\nConfusion matrix", "\n")
print(round(acctab,4))
}
invisible(acctab)
}
## Ejemplo con los datos iris
confusion(iris2$Species, predictLDA$class)
## Primer intento de obtener una estimaci�n con menor optimismo
## uso de CV=TRUE
# Lineal
confusion(iris2$Species, lda(Species ~., data=iris2,CV=TRUE)$class)
# Cuadratico
confusion(iris2$Species, qda(Species ~., data=iris2,CV=TRUE)$class)
table(iris2$Species, qda(Species ~., data=iris2,CV=TRUE)$class)
###################################
# Tarea: Repetir el an�lisis con los datos crabs de la libreria(MASS)
#lectura y creacion de 4 grupos
library(MASS)
data(crabs)
spsex <- 2 * as.numeric(crabs$sp) + as.numeric(crabs$sex)
spsex<-factor(spsex, levels=c(3,4,5,6), labels=c("b","B","o","O"))
plot(crabs[, 4:8], col = spsex)
sex
sex
data(crabs)
spsex <- 2 * as.numeric(crabs$sp) + as.numeric(crabs$sex)
spsex<-factor(spsex, levels=c(3,4,5,6), labels=c("b","B","o","O"))
plot(crabs[, 4:8], col = spsex)
densityplot( ~ FL+RW+CL+CW+BD, groups=spsex, data=crabs)
# ampliamos el conjunto de datos con las 3 primeras componentes principales
crabsPC <- predict(princomp(crabs[, 4:8]))
pairs(crabsPC[, 1:3], col = spsex)
crabs2<-cbind(crabs,spsex, crabsPC[,1:3])
names(crabs2)
mod.lda <- lda(spsex ~ Comp.1+Comp.2+Comp.3, data = crabs2)
mod.lda
library(MASS)
data(Pima.tr)
help(Pima.tr)
help(Pima)
help(Pima.tr2)
help(Pima.tr)
plot(Pima.tr[,1:7],col=Pima.tr[,8])
densityplot( ~ Pima.tr[,1:7], groups=Pima.tr$type, data=Pima.tr)
mod.lda <- lda(type ~ ., data = Pima.tr)
predictLDA <- predict(mod.lda, newdata = Pima.te)
TAB <- table(Pima.te$type, predictLDA$class)
TAB
mcrlda <- 1 - sum(diag(TAB))/sum(TAB)
mcrlda
confusion(Pima.te$type, predictLDA$class)
mod.qda <- qda(type ~ ., data = Pima.tr)
predictQDA <- predict(mod.qda, newdata = Pima.te)
TAB <- table(Pima.te$type, predictQDA$class)
TAB
mcrqda <- 1 - sum(diag(TAB))/sum(TAB)
mcrqda
confusion(Pima.te$type, predictQDA$class)
### MODELO de REGRESION LOG�STICA
pima.glm1<-glm(type~., binomial, data=Pima.tr)
summary(pima.glm1)
predictGLM<-predict(pima.glm1, newdata= Pima.te, type = "response")
TAB <- table(Pima.te$type, predictGLM > .5)
TAB
mcrglm1 <- 1 - sum(diag(TAB))/sum(TAB)
pima.glm1<-glm(type~., binomial, data=Pima.tr) # para los Pima.training
summary(pima.glm1)
predictGLM<-predict(pima.glm1, newdata= Pima.te, type = "response") # para los Pima.test
TAB <- table(Pima.te$type, predictGLM > .5) # modificacion: nos quedamos con los valores predichos > 0.5
TAB
mcrglm1 <- 1 - sum(diag(TAB))/sum(TAB)
mcrglm1
TAB <- table(Pima.te$type, round(predictGLM ))
TAB
mcrglm1 <- 1 - sum(diag(TAB))/sum(TAB)
mcrglm1
confusion(Pima.te$type, (predictGLM > .5))
## visualizacion de los efectos
termplot(pima.glm1, se=TRUE, rug=TRUE)
termplot(pima.glm1, se=TRUE, rug=TRUE)
termplot(pima.glm1, se=TRUE, rug=TRUE)
require(effects)
install.packages("effects")
require(effects)
plot(allEffects(pima.glm1))
# sin embargo el valor bp no influye, es una línea plana, o skin también
## a�adir suavizadores para detectar efecto no lineal
termplot(pima.glm1, partial.resid=TRUE, se=TRUE, rug=TRUE, smooth=panel.smooth, span.smth=1/5)
pima.glm2<-step(pima.glm1)
summary(pima.glm2)
## Ajustamos un modelo con posibles interacciones de orden 2.
stepAIC(pima.glm1, scope=c(upper=.~.^2, lower=.~1))
pima.glm3<-stepAIC(pima.glm1, scope=c(upper=.~.^2, lower=.~1))
stepAIC(pima.glm1)
pima.glm2<-step(pima.glm1)
summary(pima.glm2)
## Ajustamos un modelo con posibles interacciones de orden 2.
stepAIC(pima.glm1, scope=c(upper=.~.^2, lower=.~1))
pima.glm3<-stepAIC(pima.glm1, scope=c(upper=.~.^2, lower=.~1))
summary(pima.glm3)
pima.glm3<-stepAIC(pima.glm1, scope=c(upper=.~.^2, lower=.~1)) # interacciones de ornde 2 entre lsa variables
summary(pima.glm3)
confusion(Pima.te$type ,round(predict(pima.glm3, newdata= Pima.te, type = "response")))
# visualización de interacciones
plot(allEffects(pima.glm3))
# Script para la sesion del 28/2/14 de Introd. a la Mineria de Datos
# He tenido que terminar ejecutandolo en una versi?n 2.15 de R por
# problemas de la librer?a Rcpp en la versi?n 3.0.2 y similares.
## Necesitamo varios paquetes nuevos (tm: mineria de textos),
## ('wordcloud': nubes de palabras)
## necesito un fichero (.txt) que contiene el texto a analizar.
## He tomado algo aburrido como el debate entre Rubalcaba y Rajoy ()
## Instalar aquellas librer?as que no est?n todav?a cargadas
require(tm)
install.packages("tm")
install.packages("tm")
require(tm)
require(wordcloud)
install.packages("wordcloud")
require(RColorBrewer)
require(SnowballC)
install.packages("SnowballC")
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap) # Quantitative discourse analysis of transcripts.
install.packages("qdap")
library(dplyr) # Data preparation and pipes %>%.
library(dplyr) # Data preparation and pipes %>%.
library(RColorBrewer) # Generate palette of colours for plots.
library(ggplot2) # Plot word frequencies.
library(scales) # Include commas in numbers.
library(Rgraphviz)
install.packages("Rgraphivz")
library(Rgraphviz)
#library(Rgraphviz)
## Leer el fichero de texto, pero con cuidado de no leer palabras que no
## son sem?nticamente relevantes pero si muy frecuentes: de , por , la los,....
## El texto le?do forma el Corpus
# leemos el archive con el texto
# creamos un corpus
mensaje.corpus<-  Corpus(DirSource("txtmin/mensaje",encoding ="latin1"))
## En la versi?n 0.6 se deber?a  usar la siguiente sintaxis que asegura la herencia de atributos entre clases.
setwd("~/Master/mineria/p3/materiales TextMining")
mensaje.corpus<-  Corpus(DirSource("txtmin/mensaje",encoding ="latin1"))
## En la versi?n 0.6 se deber?a  usar la siguiente sintaxis que asegura la herencia de atributos entre clases.
mensaje.corpus <- tm_map( mensaje.corpus,contet_transformer(removePunctuation)) #
getTransformations() # visualizamos las operaciones predefinidas
## es f?cil definir nuevas operaciones adapatadas al usuario
# eliminamos puntuaciones y palabras espec?ficas
mensaje.corpus <- tm_map( mensaje.corpus,contet_transformer(removePunctuation)) #
mensaje.corpus <- tm_map( mensaje.corpus, contet_transformer(tolower))  # todo a minusculas
mensaje.corpus <- tm_map( mensaje.corpus,function(x)removeWords(x,stopwords("spanish")))
stopwords("spanish")
mensaje.corpus <- tm_map( mensaje.corpus, contet_transformer(stemDocument))
# En la version 0.6 de tm conviene asegurar la conversion al tipo de dato adecuado tras las transformaciones
# realizadas por tm_map
mensaje.corpus<-Corpus(VectorSource(mensaje.corpus))
## En la versi?n 0.5-10 no era necesario el uso de contet_transformer
# eliminamos puntuaciones y palabras espec?ficas
mensaje.corpus <- tm_map( mensaje.corpus, removePunctuation) #
mensaje.corpus <- tm_map( mensaje.corpus, tolower)  # todo a minusculas
mensaje.corpus <- tm_map( mensaje.corpus,function(x)removeWords(x,stopwords("spanish")))
stopwords("spanish")
mensaje.corpus <- tm_map(mensaje.corpus, stripWhitespace)
# Realizar o no stemming puede depender del problema concreto que se aborde.
# En este ejemplo no lo veo una mejora importante
#
# mensaje.corpus <- tm_map( mensaje.corpus, stemDocument, language = "spanish")
#
# inspeccion del corpus
inspect(mensaje.corpus)
# Construimos matrices de t?rminos y documentos que podemos utilizar alternativamente al Corpus
# Calculamos la frecuencia con que aparecen las palabras
## usamos la matriz de documentos y terminos
tdm <- TermDocumentMatrix(mensaje.corpus)
dtm <- DocumentTermMatrix(mensaje.corpus)  ## traspuesta de la anterior
## Estad?stica de frecuencias b?sica
class(dtm)
dim(dtm)
freq<-colSums(as.matrix(dtm))
freq[tail(order(freq))]
findFreqTerms(dtm, lowfreq=5)
findAssocs(dtm, "ciudadanos", corlimit=0.000)
freq<-sort(colSums(as.matrix(dtm)), decreasing=TRUE)
freq
wf<- data.frame(word=names(freq), freq=freq)
head(wf)
library(ggplot2)
subset(wf, freq>500)
ggplot(aes(word, freq))
geom_bar(stat="identity")
theme(axis.text.x=element_text(angle=45,
# primera prueba
wordcloud(mensaje.corpus, scale=c(3,0.5), max.words=200, random.order=FALSE, rot.per=0.33, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))
#Segunda forma
# Creamos  una matriz con la frecuencia de palabras
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq, min.freq=4)
nchar(as.character(d$word))
plot(density(nchar(as.character(d$word))))
# creamos un corpus
debate.corpus <- Corpus(DirSource("txtmin/"))
inspect(debate.corpus)
# eliminamos puntuaciones y palabras espec?ficas (esta sintaxis es adecuada para versiones anteriores)
debate.corpus <- tm_map( debate.corpus,removePunctuation) #
debate.corpus <- tm_map(debate.corpus, stripWhitespace)
debate.corpus <- tm_map( debate.corpus, tolower)  # todo a minusculas
debate.corpus <- tm_map( debate.corpus,function(x)removeWords(x,stopwords("spanish")))
debate.corpus <- tm_map(debate.corpus, stemDocument)
## En la versi?n 0.6 se deber?a  usar la siguiente sintaxis que asegura la herencia de atributos entre clases.
debate.corpus <- tm_map( debate.corpus,contet_transformer(removePunctuation)) #
getTransformations() # visualizamos las operaciones predefinidas
## es f?cil definir nuevas operaciones adapatadas al usuario
# eliminamos puntuaciones y palabras espec?ficas
mensaje.corpus <- tm_map( mensaje.corpus,contet_transformer(removePunctuation)) #
mensaje.corpus <- tm_map( mensaje.corpus, contet_transformer(tolower))  # todo a minusculas
mensaje.corpus <- tm_map( mensaje.corpus,function(x)removeWords(x,stopwords("spanish")))
stopwords("spanish")
mensaje.corpus <- tm_map( mensaje.corpus, stemDocument)
wordStem("Ni todos los espa?oles viven en Espa?a, ni todos los que viven en Espa?a son espa?oles", language="spanish")
# En la version 0.6 de tm conviene asegurar la conversion al tipo de dato adecuado tras las transformaciones
# realizadas por tm_map
debate.corpus<-Corpus(VectorSource(debate.corpus))
# hay palabras muy frecuentes que deber?amos eliminar
debate.corpus <- tm_map(debate.corpus, removeWords, c("usted", "ustedes", "se?or","rajoy", "rubalcaba", "campo", "p?rez"))
#Segunda forma
# Construimos matrices de t?rminos que podemos utilizar alternativamente al Corpus
# Calculamos la frecuencia con que aparecen las palabras
# una matriz es la traspuesta de la otra
tdm <- TermDocumentMatrix(debate.corpus)
tdm <- TermDocumentMatrix(debate.corpus)
dtm <- DocumentTermMatrix(debate.corpus)
# Creamos  una matriz con la frecuencia de palabras
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq, min.freq=1)
# Tarea Abierta
# creamos con el texto del debate tres textos uno para cada interveniente
# (comienzan en mayusculas)
#
x<-debate.corpus[[1]]
inspect(x)
# Idea: aprovechar la libreria qdap para "trocear" el debate.
# Ordenes para hacer algo parecido en un debate americano
library(qdap)
dat <- unlist(strsplit(x, "\\n"))
locs <- grep("STATEMENT OF ", dat)
nms <- sapply(strsplit(dat[locs], "STATEMENT OF |,"), "[", 2)
dat[locs] <- "SPLIT_HERE"
corp <- with(data.frame(person=nms, dialogue =
Trim(unlist(strsplit(paste(dat[-1], collapse=" "), "SPLIT_HERE")))),
df2tm_corpus(dialogue, person))
tm::inspect(corp)
debate.corpus <- Corpus(DirSource("txtmin/"))
inspect(debate.corpus)
# eliminamos puntuaciones y palabras espec?ficas (esta sintaxis es adecuada para versiones anteriores)
debate.corpus <- tm_map( debate.corpus,removePunctuation) #
debate.corpus <- tm_map(debate.corpus, stripWhitespace)
debate.corpus <- tm_map( debate.corpus, tolower)  # todo a minusculas
debate.corpus <- tm_map( debate.corpus,function(x)removeWords(x,stopwords("spanish")))
debate.corpus <- tm_map(debate.corpus, stemDocument)
## En la versi?n 0.6 se deber?a  usar la siguiente sintaxis que asegura la herencia de atributos entre clases.
debate.corpus <- tm_map( debate.corpus,contet_transformer(removePunctuation)) #
getTransformations() # visualizamos las operaciones predefinidas
## es f?cil definir nuevas operaciones adapatadas al usuario
# eliminamos puntuaciones y palabras espec?ficas
mensaje.corpus <- tm_map( mensaje.corpus,contet_transformer(removePunctuation)) #
mensaje.corpus <- tm_map( mensaje.corpus, contet_transformer(tolower))  # todo a minusculas
mensaje.corpus <- tm_map( mensaje.corpus,function(x)removeWords(x,stopwords("spanish")))
stopwords("spanish")
mensaje.corpus <- tm_map( mensaje.corpus, stemDocument)
wordStem("Ni todos los espa?oles viven en Espa?a, ni todos los que viven en Espa?a son espa?oles", language="spanish")
# En la version 0.6 de tm conviene asegurar la conversion al tipo de dato adecuado tras las transformaciones
# realizadas por tm_map
debate.corpus<-Corpus(VectorSource(debate.corpus))
# hay palabras muy frecuentes que deber?amos eliminar
debate.corpus <- tm_map(debate.corpus, removeWords, c("usted", "ustedes", "se?or","rajoy", "rubalcaba", "campo", "p?rez"))
#Segunda forma
# Construimos matrices de t?rminos que podemos utilizar alternativamente al Corpus
# Calculamos la frecuencia con que aparecen las palabras
# una matriz es la traspuesta de la otra
tdm <- TermDocumentMatrix(debate.corpus)
tdm <- TermDocumentMatrix(debate.corpus)
dtm <- DocumentTermMatrix(debate.corpus)
# Creamos  una matriz con la frecuencia de palabras
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq, min.freq=1)
# Tarea Abierta
# creamos con el texto del debate tres textos uno para cada interveniente
# (comienzan en mayusculas)
#
x<-debate.corpus[[1]]
inspect(x)
# Idea: aprovechar la libreria qdap para "trocear" el debate.
# Ordenes para hacer algo parecido en un debate americano
library(qdap)
dat <- unlist(strsplit(x, "\\n"))
locs <- grep("STATEMENT OF ", dat)
nms <- sapply(strsplit(dat[locs], "STATEMENT OF |,"), "[", 2)
dat[locs] <- "SPLIT_HERE"
corp <- with(data.frame(person=nms, dialogue =
Trim(unlist(strsplit(paste(dat[-1], collapse=" "), "SPLIT_HERE")))),
df2tm_corpus(dialogue, person))
tdm <- TermDocumentMatrix(debate.corpus)
dtm <- DocumentTermMatrix(debate.corpus)
# Creamos  una matriz con la frecuencia de palabras
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq, min.freq=1)
# Tarea Abierta
# creamos con el texto del debate tres textos uno para cada interveniente
# (comienzan en mayusculas)
#
x<-debate.corpus[[1]]
inspect(x)
# Idea: aprovechar la libreria qdap para "trocear" el debate.
# Ordenes para hacer algo parecido en un debate americano
library(qdap)
dat <- unlist(strsplit(x, "\\n"))
locs <- grep("STATEMENT OF ", dat)
nms <- sapply(strsplit(dat[locs], "STATEMENT OF |,"), "[", 2)
dat[locs] <- "SPLIT_HERE"
corp <- with(data.frame(person=nms, dialogue =
Trim(unlist(strsplit(paste(dat[-1], collapse=" "), "SPLIT_HERE")))),
df2tm_corpus(dialogue, person))
tm::inspect(corp)
debate.corpus <- Corpus(DirSource("txtmin/"))
inspect(debate.corpus)
# eliminamos puntuaciones y palabras espec?ficas (esta sintaxis es adecuada para versiones anteriores)
debate.corpus <- tm_map( debate.corpus,removePunctuation) #
debate.corpus <- tm_map(debate.corpus, stripWhitespace)
debate.corpus <- tm_map( debate.corpus, tolower)  # todo a minusculas
debate.corpus <- tm_map( debate.corpus,function(x)removeWords(x,stopwords("spanish")))
debate.corpus <- tm_map(debate.corpus, stemDocument)
## En la versi?n 0.6 se deber?a  usar la siguiente sintaxis que asegura la herencia de atributos entre clases.
debate.corpus <- tm_map( debate.corpus,contet_transformer(removePunctuation)) #
getTransformations() # visualizamos las operaciones predefinidas
## es f?cil definir nuevas operaciones adapatadas al usuario
# eliminamos puntuaciones y palabras espec?ficas
mensaje.corpus <- tm_map( mensaje.corpus,contet_transformer(removePunctuation)) #
mensaje.corpus <- tm_map( mensaje.corpus, contet_transformer(tolower))  # todo a minusculas
mensaje.corpus <- tm_map( mensaje.corpus,function(x)removeWords(x,stopwords("spanish")))
stopwords("spanish")
mensaje.corpus <- tm_map( mensaje.corpus, stemDocument)
wordStem("Ni todos los espa?oles viven en Espa?a, ni todos los que viven en Espa?a son espa?oles", language="spanish")
# En la version 0.6 de tm conviene asegurar la conversion al tipo de dato adecuado tras las transformaciones
# realizadas por tm_map
debate.corpus<-Corpus(VectorSource(debate.corpus))
# hay palabras muy frecuentes que deber?amos eliminar
debate.corpus <- tm_map(debate.corpus, removeWords, c("usted", "ustedes", "se?or","rajoy", "rubalcaba", "campo", "p?rez"))
#Segunda forma
# Construimos matrices de t?rminos que podemos utilizar alternativamente al Corpus
# Calculamos la frecuencia con que aparecen las palabras
# una matriz es la traspuesta de la otra
tdm <- TermDocumentMatrix(debate.corpus)
tdm <- TermDocumentMatrix(debate.corpus)
dtm <- DocumentTermMatrix(debate.corpus)
# Creamos  una matriz con la frecuencia de palabras
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud(d$word,d$freq, min.freq=1)
# Tarea Abierta
# creamos con el texto del debate tres textos uno para cada interveniente
# Script para la sesion del 28/2/14 de Introd. a la Mineria de Datos
# He tenido que terminar ejecutandolo en una versi?n 2.15 de R por
# problemas de la librer?a Rcpp en la versi?n 3.0.2 y similares.
## Necesitamo varios paquetes nuevos (tm: mineria de textos),
## ('wordcloud': nubes de palabras)
## necesito un fichero (.txt) que contiene el texto a analizar.
## He tomado algo aburrido como el debate entre Rubalcaba y Rajoy ()
## Instalar aquellas librer?as que no est?n todav?a cargadas
require(tm)
require(wordcloud)
require(RColorBrewer)
require(SnowballC)
library(tm) # Framework for text mining.
library(SnowballC) # Provides wordStem() for stemming.
library(qdap) # Quantitative discourse analysis of transcripts.
library(qdapDictionaries)
install.packages("qdap")
