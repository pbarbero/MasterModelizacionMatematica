{
    "contents" : "\n# Modelo simulado\n\n###############\n\nset.seed(353)\nN = 100\nx = sort(runif(N))\ny = rnorm(N,sin(4*x),1/3)\nplot(x,y,main=\"Regresi?n y Suavizadores\")\nx2 = seq(0,1,length=1000)\nlines(x2,sin(4*x2))\nd = data.frame(x=x,y=y)\n\n#######################\n# Modelado param?trico\n#######################\n\n\n## funci?n que construye la matriz del dise?o asociado a polinomios trigonometricos\n\n#\n# Periodic model using sines and cosines\n#\n\n\n    cs <- function(t,harmonics=1) {\n       # generate matrix with sin,cos pairs with period 1\n       ret <- numeric(0)\n       for ( i in harmonics) {\n           ret <- cbind( ret, cos(2*pi*i*t), sin(2*pi*i*t))\n       }\n       if (missing(harmonics)) cnames <- c('c','s')\n       else {\n            cnames <- paste( c(\"c\",\"s\"), rep(harmonics, each = 2),sep=\".\")\n       }\n       colnames(ret) <- cnames\n       ret\n    }\n\n\n# Ajustes con diferente grado de complejidad\n\n    fit.cs= lm(y~cs(x),d)\n\n    summary(fit.cs)   # can we drop a term??\n    anova(fit.cs)     # here anova respects 'marginality'\n    plot( d$x, d$y)\n    dtest=data.frame(x=x2)\n    lines( x2 , predict(fit.cs,dtest),col='red')\n\n    lines( x2, predict( lm( y ~ cs(x,1:4),d),dtest),col='green')\n\n\n    lines( x2, predict( lm( y ~ cs(x,1:8),d),dtest), col = 'black',lwd=1.5)\n\n\n    lines( x2, predict( lm( y ~ cs(x,1:18),d),dtest), col = 'blue',lwd=1.5)\n\n    lines( x2, predict( fit.cs <- lm( y ~ cs(x,1:50),d),dtest), col = 'purple',lwd=1.5)\n    summary(fit.cs)\n\n# seleccion del mejor modelo\n\n## Using adjusted R2\n\n    require(stats4)\n\n\n    measures <- matrix(NA,nrow=50,ncol=8,\n             ,dimnames=list(NULL,c(\"H\",\"df\",\"R.sq\",\"Adj.R.sq\",\"logLik\",\n             \"-2logLik\",\"AIC\",\"BIC\")))\n\n\n\n    for ( maxh in 1:50) {\n       fs <- summary( fit <- lm( y ~ cs(x,1:maxh), d))\n       measures[maxh,] <- c(maxh, fit$df[1],100*fs$r.squared , 100*fs$adj.r.squared,logLik(fit),\n                          -2*logLik(fit),  AIC(fit), BIC(fit))\n    }\n\n    round(measures,2)\n\n## seleccion del n?mero de harmonicos por CV\n\nlibrary(boot)\n\n\n   eep <- rep(NA,20)\n    for ( harmonic in 1:20 )  {\n          eep[harmonic] <- cv.glm( d, glm( y ~ cs(x, 1:harmonic), data = d), K = 10)$delta[2]\n    }\n    \n    plot(eep,type = 'b')\n\n\n## minimum around 2   ????\n\n# fit model using all data\n\n    fit.cv <- lm( y ~ cs(x,1:2), d)\n    plot( d$x, d$y)\n    lines( d$x, predict(fit.cv))\n\n\n## ajuste polinomial seleccionando el grado por CV\n\n\n   eep <- rep(NA,20)\n    for ( grado in 1:20 )  {\n          eep[grado] <- cv.glm( d, glm( y ~ poly(x, grado), data = d), K = 10)$delta[2]\n    }\n    \n    plot(eep,type = 'b')\n\n# minimo en 7??\n\n    fit.cv <- lm( y ~ poly(x,7), d)\n    plot( d$x, d$y)\n    lines( d$x, predict(fit.cv))\n\n## Otras bases de funciones: ns (splines naturales)\n\n  library(splines)\n  eep <- rep(NA,20)\n    for ( grado in 1:20 )  {\n          eep[grado] <- cv.glm( d, glm( y ~ ns(x, grado), data = d), K = 10)$delta[2]\n    }\n    \n    plot(eep,type = 'b')\n\n   fit.cv <- lm( y ~ ns(x,3), d)\n    plot( d$x, d$y)\n    lines( d$x, predict(fit.cv))\n\n#########################\n# Modelado No param?trico\n#########################\n\n## Ejemplo de Funci?n en R.\n## Realiza una regresi?n de tipo k-nn \n# como argumentos de entrada, los vectores xtrain, ytrain, el entero k (por defecto 1) y xtest como\n# vector donde predecir\n\n# la funci?n devuelve un vector con los valores predichos en xtest\n\nknnreg = function(xtrain,ytrain,xtest,k=1)\n  {\n    n = length(xtrain)\n    N = length(xtest)\n    f.hat = rep(NA,N)\n    for(i in 1:N)\n      {\n        d = (xtest[i]-xtrain)^2\n        ind = c(1:n)[rank(d)<(k+1)]\n        f.hat[i] = mean(y[ind])\n      }\n    f.hat\n  }\n    \n###############\n\n# Modelos No-Param?tricos\n\n\n\n#ajuste local constante (equiv al estimador de Nadaraya-Watson)\n\nfit = loess(y~x,deg=0,span=0.4,data=d)\npred = predict(fit,d)\nlines(hex,pred,lty=2)\nsummary(fit)\n\n#Call:\n\n#loess(formula = y ~ x, data = d, span = 0.4, degree = 0)\n#\n#Number of Observations: 100 \n#Equivalent Number of Parameters: 3.63 \n#Residual Standard Error: 0.3633 \n#Trace of smoother matrix: 4.23 \n#\n#Control settings:\n#  normalize:  TRUE \n#  span     :  0.4 \n#  degree   :  0 \n#  family   :  gaussian\n#  surface  :  interpolate    cell = 0.2\n\n# Ajuste por regresi?n mediante K-vecinos proximos, con k elegido en base al estimador de N-W\n\nk=as.integer(100/4.23)\npred.knn = knnreg(x,y,x,k=k)\nlines(x,pred.knn,lty=2)\n\n# Ajuste localmente lineal (d=1)\n\nfit.lp = loess(y~x,deg=1,span=0.5,data=d)\nsummary(fit.lp)\n#Call:\n#loess(formula = y ~ x, data = d, span = 0.5, degree = 1)\n#\n#Number of Observations: 100 \n#Equivalent Number of Parameters: 3.67 \n#Residual Standard Error: 0.3404 \n#Trace of smoother matrix: 4.28 \n#\n#Control settings:\n#  normalize:  TRUE \n#  span     :  0.5 \n#  degree   :  1 \n#  family   :  gaussian\n#  surface  :  interpolate    cell = 0.2\npred.lp = predict(fit.lp,d)\nlines(x,pred.lp,lty=3)\nlegend(0.2,-0.5,legend=c(\"True\",\"N-W\",\"knn18\",\"loc-reg\"),lty=1:4)\n\nlibrary(splines)\nlibrary(ElemStatLearn)\n\n## El paquete de funciones ElemStatLearn contiene un numero importante de funciones y la mayor?a\n## de los conjuntos de datos del libro de Hastie y Tibshirani\n\n\nfit.spline<-smooth.spline(d$x,d$y)\nlines(fit.spline, col=3)\n\nlibrary(mgcv) # biblioteca de GAM's\n\nfit.gam<-gam(y~s(x), data=d)\nplot(fit.gam)\n\n\n# Otra librer?a de uso frecuente en suavizados\nlibrary(KernSmooth)\nhelp(locpoly)\n\n\n\n\n#############################################################################3\n# Propuesta: Analizar los datos motorcycle.  \n# Utiliza loess, knnreg, smoothin.spline, gam, etc... \n#############################################################################\nlibrary(MASS)\ndata(mcycle)\nplot(mcycle)\nnames(mcycle)\n\n# ejemplo\n#library(splines)\n#lines(mcycle$times,lm(accel~ns(times,3), data=mcycle)$fit)\n\n\n###################################################\n# Selecci?n autom?tica de variables en regresion\n######################################################\n\nhelp(step)\n\nlibrary(ElemStatLearn) # En esta biblioteca se encuentra el conjunto de datos  prostate\n\nhelp(prostate)\n\npairs(prostate[1:8])\n\n# prostate = read.table(\"../../data/prostate.data\")\nprostate.lm = lm(lpsa~.-train,data=prostate)\nstep(prostate.lm)\n#Start:  AIC=-58.32\n#lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + \n#    pgg45\n#\n#          Df Sum of Sq     RSS     AIC\n#- gleason  1     0.041  44.204 -60.231\n#- pgg45    1     0.526  44.689 -59.174\n#- lcp      1     0.674  44.837 -58.852\n#<none>                  44.163 -58.322\n#- age      1     1.550  45.713 -56.975\n#- lbph     1     1.684  45.847 -56.693\n#- lweight  1     3.586  47.749 -52.749\n#- svi      1     4.935  49.099 -50.045\n#- lcavol   1    22.372  66.535 -20.567\n#\n#Step:  AIC=-60.23\n#lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45\n#\n#          Df Sum of Sq     RSS     AIC\n#- lcp      1     0.662  44.867 -60.788\n#<none>                  44.204 -60.231\n#- pgg45    1     1.192  45.396 -59.650\n#- age      1     1.517  45.721 -58.959\n#- lbph     1     1.705  45.910 -58.559\n#- lweight  1     3.546  47.751 -54.746\n#- svi      1     4.898  49.103 -52.037\n#- lcavol   1    23.504  67.708 -20.872\n#\n#Step:  AIC=-60.79\n#lpsa ~ lcavol + lweight + age + lbph + svi + pgg45\n#\n#          Df Sum of Sq     RSS     AIC\n#- pgg45    1     0.659  45.526 -61.374\n#<none>                  44.867 -60.788\n#- age      1     1.265  46.132 -60.092\n#- lbph     1     1.647  46.513 -59.293\n#- lweight  1     3.565  48.431 -55.373\n#- svi      1     4.250  49.117 -54.009\n#- lcavol   1    25.419  70.286 -19.248\n#\n#Step:  AIC=-61.37\n#lpsa ~ lcavol + lweight + age + lbph + svi\n#\n#          Df Sum of Sq     RSS     AIC\n#<none>                  45.526 -61.374\n#- age      1     0.959  46.485 -61.352\n#- lbph     1     1.857  47.382 -59.497\n#- lweight  1     3.225  48.751 -56.735\n#- svi      1     5.952  51.477 -51.456\n#- lcavol   1    28.767  74.292 -15.870\n#\n#Call:\n#lm(formula = lpsa ~ lcavol + lweight + age + lbph + svi, data = prostate)\n#\n#Coefficients:\n#(Intercept)       lcavol      lweight          age         lbph          svi  \n#0.95102      0.56561      0.42369     -0.01489      0.11184      0.72096\n\n##\n# Seleccion utilizando BIC en lugar de AIC\n######\n\nlog(sum(prostate$train))\n\n\nstep(prostate.lm, k=log(sum(prostate$train)))\n\n#Start:  AIC=-38.48\n#lpsa ~ (lcavol + lweight + age + lbph + svi + lcp + gleason + \n#    pgg45 + train) - train\n#\n#          Df Sum of Sq     RSS     AIC\n#- gleason  1     0.041  44.204 -42.594\n#- pgg45    1     0.526  44.689 -41.536\n#- lcp      1     0.674  44.837 -41.215\n#- age      1     1.550  45.713 -39.337\n#- lbph     1     1.684  45.847 -39.055\n#<none>                  44.163 -38.479\n#- lweight  1     3.586  47.749 -35.111\n#- svi      1     4.935  49.099 -32.408\n#- lcavol   1    22.372  66.535  -2.929\n#\n#Step:  AIC=-42.59\n#lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45\n#\n#         Df Sum of Sq     RSS     AIC\n#- lcp      1     0.662  44.867 -45.356\n#- pgg45    1     1.192  45.396 -44.217\n#- age      1     1.517  45.721 -43.526\n#- lbph     1     1.705  45.910 -43.127\n#<none>                  44.204 -42.594\n#- lweight  1     3.546  47.751 -39.313\n#- svi      1     4.898  49.103 -36.604\n#- lcavol   1    23.504  67.708  -5.439\n#\n#Step:  AIC=-45.36\n#lpsa ~ lcavol + lweight + age + lbph + svi + pgg45\n#\n#          Df Sum of Sq     RSS     AIC\n#- pgg45    1     0.659  45.526 -48.146\n#- age      1     1.265  46.132 -46.864\n#- lbph     1     1.647  46.513 -46.064\n#<none>                  44.867 -45.356\n#- lweight  1     3.565  48.431 -42.145\n#- svi      1     4.250  49.117 -40.781\n#- lcavol   1    25.419  70.286  -6.020\n#\n#Step:  AIC=-48.15\n#lpsa ~ lcavol + lweight + age + lbph + svi\n#\n#          Df Sum of Sq     RSS     AIC\n#- age      1     0.959  46.485 -50.328\n#- lbph     1     1.857  47.382 -48.473\n#<none>                  45.526 -48.146\n#- lweight  1     3.225  48.751 -45.712\n#- svi      1     5.952  51.477 -40.433\n#- lcavol   1    28.767  74.292  -4.847\n#\n#Step:  AIC=-50.33\n#lpsa ~ lcavol + lweight + lbph + svi\n#\n#          Df Sum of Sq     RSS     AIC\n#- lbph     1     1.300  47.785 -51.857\n#<none>                  46.485 -50.328\n#- lweight  1     2.801  49.286 -48.857\n#- svi      1     5.806  52.291 -43.116\n#- lcavol   1    27.830  74.315  -9.022\n#\n#Step:  AIC=-51.86\n#lpsa ~ lcavol + lweight + svi\n#\n#          Df Sum of Sq     RSS     AIC\n#<none>                  47.785 -51.857\n#- svi      1     5.181  52.966 -46.076\n#- lweight  1     5.892  53.677 -44.783\n#- lcavol   1    28.045  75.830 -11.270\n#\n#Call:\n#lm(formula = lpsa ~ lcavol + lweight + svi, data = prostate)\n#\n#Coefficients:\n#(Intercept)       lcavol      lweight  \n#    -0.2681       0.5516       0.5085  \n#        svi  \n#     0.6662  \n\n##\n# Acudir a la ayuda de prostate en ElemStatLearn\n\nhelp(prostate)\n\n# Realizamos una regresi?n ridge (o contraida)\n\npairs( prostate[,1:9], col=\"violet\" )\ntrain <- subset( prostate, train==TRUE )[,1:9]\ntest  <- subset( prostate, train=FALSE )[,1:9]\n\n\nprostate.ridge <- simple.ridge( train[,1:8], train[,9], df=1:8 )\n#\n# coefficient traces:\n#\nmatplot( prostate.ridge$df, t(prostate.ridge$beta), type=\"b\", \n        col=\"blue\", pch=17, ylab=\"coefficients\" )\n\n# Valor seleccionado para el parametro lambda\nrequire(MASS)\nlm.ridge(lpsa~., data=train)$kHKB\n\n\n###\n## Ajustamos el modelo mediante un GAM\n##\n\nprostate.gam<-gam(lpsa~s(lcavol)+s(lweight)+s(age)+s(lbph)+svi+s(lcp)+gleason+s(pgg45), data=train)\n\nsummary(prostate.gam)\n\nplot(prostate.gam)\n\n\nstep(prostate.gam)\n\n## Utilizamos la libreria leaps que \n## incluye una funcion regsubset de seleccion automatica\n## de los nbest mejores modelos para cada uno de los tama?os\n## de 1 a nvmax variables con el criterio BIC \n\n\nlibrary(leaps)\n\n# Si no se ha cargado anteriormente:\nlibrary(ElemStatLearn)\n\n\n\nhelp(prostate)\nnames(prostate)\nattach(prostate)\n\n\nlm.prostate<-lm(lpsa~.-train,data=prostate)\nstep(lm.prostate)   # seleccion por AIC\n\nstep(lm.prostate, k=log(length(prostate$lpsa)))\n\n\nlm.regsubset2 <- regsubsets(lpsa ~ .-train, data = prostate, nbest = 2, nvmax=8)\nplot(lm.regsubset2)\n\n\n\n## Ajustamos un modelo para predecir el lcavol\n\nlm.regsubset2 <- regsubsets(lcavol ~ .-train, data = prostate, nbest = 2, nvmax=8)\nplot(lm.regsubset2)\n\n\nplot(lcavol~lpsa, data=prostate)\n\n## permitimos interacci?n entre las variables\n\nlm.regsubset3 <- regsubsets(lcavol ~ (.-train)^2, data = prostate, nbest = 2, nvmax=8)\nplot(lm.regsubset3)\n\nstep(lm(lcavol~(.-train)^2,data=prostate), k=log(length(prostate$lpsa)))\n\nlm.calvol.bic<-step(lm(lcavol~(.-train)^2,data=prostate), k=log(length(prostate$lpsa)))\n\n\nplot(lcavol~lpsa, data=prostate)\n\n## podemos analizar visualmente el efecto de la interacci?n de dos variables\n\ncoplot(lcavol ~ lpsa | lcp, data = prostate)\ngiven.lcp <- co.intervals(prostate$lcp, number=2, overlap=.1)\ncoplot(lcavol ~ lpsa | lcp, data = prostate, given.v=given.lcp, rows=1)\n\n## El paquete effects tb. facilita la representacion de efectos en \n## modelos lineales generales de forma c?moda\n## Se carga automaticamente dentro de Rcmdr\n\nlibrary(effects)\nplot(allEffects(lm.calvol.bic))\n\n## Probad con \n### plot(allEffects(gam(lcavol~ ns(lpsa,2)*lcp, data=prostate)))\n\n\n",
    "created" : 1423227249158.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "71972233",
    "id" : "945C5382",
    "lastKnownWriteTime" : 1423243328,
    "path" : "~/Master/mineria/p1/md160209.r",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}