\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{tikz} 
\usepackage[utf8]{inputenc}
%\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{exercise}

\author{Pilar Barbero Iriarte}

\newenvironment{exercise}[1]% environment name
{% begin code
  \par\vspace{\baselineskip}\noindent
  \textbf{Ejercicio (#1)}\begin{itshape}%
  \par\vspace{\baselineskip}\noindent\ignorespaces
}%
{% end code
  \end{itshape}\ignorespacesafterend
}

\usepackage{Sweave}
\begin{document}
\input{tarea-concordance}


\begin{titlepage}
\begin{center}


% Upper part of the page. The '~' is needed because \\
% only works if a paragraph has started.

\textsc{\LARGE M\'aster en Modelizaci\'on \\e Investigaci\'on Matem\'atica,\\ Estad\'istica y Computaci\'on }\\[1.5cm]
{\large \today}

\textsc{Tarea de regresi\'on}\\[0.5cm]

% Title
\vfill

{ \huge \bfseries Miner\'ia de datos \\[0.4cm] }
\vfill

\includegraphics[width=0.5\textwidth]{logoUZ.png}~\\[1cm]

% Author and supervisor
\noindent
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Autor:}\\
Pilar Barbero Iriarte
\end{flushleft}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Profesor:} \\
Tom\'as Alcal\'a Nalvaiz
\end{flushright}
\end{minipage}

% Bottom of the page
\end{center}


\end{titlepage}

\pagebreak
\tableofcontents
\pagebreak


\section{Presentaci\'on de los datos}

\begin{exercise}{a}

En el paquete de funciones $MASS$ encontramos un conjunto de datos interesante
para ensayar diversos t\'ecnicas de regresi\'on. El conjunto se denomina
$mcycle$. Recogen mediciones del tiempo tras un impacto y de la aceleraci\'on
de la cabeza en simulacros de accidentes de moto, orientados al dise\~no de
cascos. Cargamos los datos y vemos el gr\'afico de dispersi\'on correspondiente.

\end{exercise}

\bigskip

El siguiente conjunto de datos ha sido proporcionado con el fin de medir la aceleración de la cabeza en un accidente de moto simulado, con el fin de probar cascos protectores. Aproximadamente en el segundo $20$ se produce el accidente, es ahí donde la regresión cobra más importancia, con el fin de poder predecir resultados.

\includegraphics{tarea-1}

\pagebreak

\section{Regresi\'on}


\begin{exercise}{b}
Como se aprecia la figura, la relaci\'on entre el tiempo y la aceleraci\'on
no es una funci\'on inmediata. Ensaya diversas aproximaciones a la misma.
En todo los casos da una estimaci\'on lo m\'as correcta posible sobre el error de
predicci\'on que se consigue con cada modelo. Utiliza para ello validaci\'on cruzada. (leve-one-out o $10-$fold).

Entre las t\'ecnicas de regresi\'on elige y compara
una de tipo param´etrico y una de tipo no param\'etrico.
\end{exercise}

\subsection{Paramétrica}

\subsubsection{Modelo de regresi\'on polin\'omica}

El modelo de regresión polinómica paramétrico tiene la forma,

\begin{equation*}
y_i = \alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 +  \dots + \alpha_d x_i^d + \epsilon_i\text{,}
\end{equation*}

donde $\epsilon_i$ es el error cometido.\\

Utilizando el m\'etodo de validaci\'on cruzada, hallaremos el grado \'optimo para la regresi\'on. Nuestro polinomio ha de tener grado mayor o igual que 2 (con grado 0 o 1 no podríamos aproximar bien el pico que se produce en el momento del accidente). 

Con el fin de hallar el grado de nuestro polinomio, realizaremos una validación cruzada sobre nuestros datos, en concreto, la validación $10-$fold que divide nuestro conjunto de datos en $10$ subconjuntos. El primer subconjunto no servirá de conjunto de test y los otros 9 nos servirán de entrenamiento. 

Este proceso se repite 10 veces, y en cada iteración, se obtiene un error. El error "absoluto" de este método es la media aritmética de todos los errores.

Supondremos un polinomio de hasta grado 20 y hallaremos el grado real sobre el que haremos la regresión:\\


\begin{Schunk}
\begin{Sinput}
> library(boot)
> set.seed(1111)
> x <- mcycle$times
> y <- mcycle$accel
> d <- data.frame(x=x,y=y)
> eep <- rep(NA,20)
> for ( grado in 1:20 ){
+ eep[grado] <- cv.glm( d, glm( y ~ poly(x, grado), data = d), 
+                      K = 10)$delta[2]
+ }
> plot(eep,type = 'b')
\end{Sinput}
\end{Schunk}
\includegraphics{tarea-b}


Se puede intuir m\'as o menos el grado del cual haremos la regresi\'on, pero lo aseguramos con:

\begin{Schunk}
\begin{Sinput}
> order(eep)
\end{Sinput}
\begin{Soutput}
 [1] 15 12  8  6  5  7  4  3 13 10  2  1  9 11 16 17 14 18 20 19
\end{Soutput}
\end{Schunk}

Nuestro polinomio va a tener grado $15$ (la primera componente del vector).

\includegraphics{tarea-d}

Gracias a utilizar la validaci\'on cruzada, aseguramos que no hay sobreajuste, es decir, nuestra funci\'on de regresi\'on no pasa por absolutamente todos los datos.

\pagebreak


\subsection{No paramétrica}

\subsubsection{Splines}

Vamos a utilizar la función de R $smooth.spline()$ la cual genera una sucesión de funciones que luego utilizaremos para hacer mínimos cuadrados. Éste método divide nuestra región en subregiones en las cuales se aplica un modelo de regresión lineal o cuadrático según convenga y luego son unidos a través de algunos puntos en concreto de nuestro conjunto de datos. 

\begin{Schunk}
\begin{Sinput}
> fit.spline <-smooth.spline(x,y)
> plot(x, y, main='Splines')
> lines(predict(fit.spline), col='blue')
\end{Sinput}
\end{Schunk}
\includegraphics{tarea-e}

Podemos intuir a simple vista que este método no va a ser muy fiable ya que depende de la disposición de nuestro conjunto de datos, y de la cantidad empleados. En caso de tener algunos datos muy concentrados, podemos recurrir a ponderar los nodos.

\subsubsection{LOESS}

LOESS es un método de regresión no paramétrico que combina múltiples modelos de regresión de tipo KNN.

Utilizaremos la función \textit{loess()} de R, sin embargo, debemos fijar los valores del \textit{span} y el grado \text{deg}. A mayor valor del span, mayor suavizado obtendremos. 

El parámetro \textit{span} controla el grado de suavizado de la regresión, mientras que el grado \textit{deg} controla el grado de los polinomios que utilizaremos para la regresión. Este valor varía entre 0, 1 y 2, aunque si utilizamos grado 0 estaremos haciendo una regresión local constante.

En este caso hemos elegido como \textit{span} igual a 0.2 y el grado será 2. 

\begin{Schunk}
\begin{Sinput}
> set.seed(1111)
> fit.lp = loess(y~x,deg=2,span=0.2,data=d)
> plot(x, y, main='LOESS')
> lines(x, predict(fit.lp), col='green')
\end{Sinput}
\end{Schunk}
\includegraphics{tarea-f}

\subsection{Comparación de los modelos}

Podemos observar aquí las tres regresiones,

\includegraphics{tarea-g}

El criterio AIC nos proporciona una técnica para decidir entre nuestros modelos estadísticos el que mejor se ajusta a nuestros datos sin conseguir un sobreajuste indeseado. En el caso general la definición del AIC es:

$$ AIC = 2k - 2 log(L)$$

donde $L$ es el estimador de máxima verosimilutud.

\begin{Schunk}
\begin{Sinput}
> sigma <- sum(residuals(fit.spline)^2)/133
> AIC_spline <- 133*(log(2*pi) + log(sigma) + 1) + 2*fit.spline$df
> sigma2 <- sum(residuals(fit.cv)^2)/133
> AIC_pol <- 133*(log(2*pi) + log(sigma2) + 1) + 2*fit.cv$df
> sigma3 <- sum(fit.lp$residuals^2)/fit.lp$n
> AIC_pl<- fit.lp$n*(log(2*pi)+log(sigma3) + 1) + 2 *fit.lp$enp
> AIC_spline
\end{Sinput}
\begin{Soutput}
[1] 1219.147
\end{Soutput}
\begin{Sinput}
> AIC_pol
\end{Sinput}
\begin{Soutput}
[1] 1427.194
\end{Soutput}
\begin{Sinput}
> AIC_pl
\end{Sinput}
\begin{Soutput}
[1] 1220.154
\end{Soutput}
\end{Schunk}

A la vista de los resultados, podemos afirmar que el menor AIC lo genera el modelo de suavizado por splines, así que nos quedamos con este.


\end{document}

